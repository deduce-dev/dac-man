\begin{figure}
  \centering
  \includegraphics[width=0.43\textwidth]{figs/synthetic-fast.pdf}
  \caption{\small Synthetic (Gordon) - effects of dynamic resource scaling using \systemname (a) Runtime, (b) Core-hours usage. \systemname scales from 64 to 128  cores and achieves better performance than running the workflow over 64 cores. }
%    \caption{\small Synthetic (Gordon). Figure compares the effects of dynamic resource scaling using E-HPC and shows (a) Turn-around time  and (b) Core-hour usage. E-HPC facilitates scaling from 64 to 128 cores and thus is able to achieve better performance than just running workflow at 64 cores. }
  \label{fig:elastic_demand}
  \vspace{-0.4cm}
\end{figure}

\subsection{\systemname Elasticity}

Figure \ref{fig:elastic_demand} shows the benefits of \systemname for
dynamic resource scaling of an application. In this experiment, we run
the synthetic parallel stage on 64 cores, 128 cores and scaling from
64 to 128 cores and we use fast mode to minimize the wait times. The
fast mode allows applications to continue making progress while other
resources are requested. In Figure \ref{fig:elastic_demand}, the \systemname
calculated bar is generated by taking the data from running \systemname in
fast mode and adding the queue time for the second job. We see that
queue times are not substantial in this case. However, \systemname in the fast mode
provides even more significant benefits in cases where queue times
are significant. 


%As shown in Figure \ref{fig:elastic_demand}, 
%the Fast Mode allows for faster turnaround even when queue times 
%are accounted for and checkpoint overhead is substantial. This is due to 
%the ability to mitigate queue time as a factor in turnaround time.

%while utilizing 12 percent more core hours than
%having originally requested 128 cores. 

Figure \ref{fig:elastic_demand}a shows that fast mode results in around 30
percent improvement in workflow runtime as compared to maintaining the
resources at 64 cores. The application is able to benefit from the
added cores and complete the application sooner. Figure
\ref{fig:elastic_demand}b shows corresponding core-hours expenditure
for each run.


%To account for pace of execution, the time spent
%running in the smaller resource set was moved to the second stage and
%normalized to the new resources.  This effectively doubled the pace of
%the application. With these adjustements it is observed that the Fast
%Mode of E-HPC effectively beats the standard version of E-HPC, and
%Fast Mode benefits from longer queue times over the standard E-HPC
%method.




